{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "stoi = {chr(i + ord(\"a\")): i + 1 for i in range(26)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {v: k for k, v in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "def build_dataset(words):\n",
    "    x = []\n",
    "    y = []\n",
    "    for name in words:\n",
    "        curr = \".\" * block_size\n",
    "        word = name + \".\"\n",
    "        for c in word:\n",
    "            x.append([stoi[i] for i in curr])\n",
    "            y.append(stoi[c])\n",
    "            curr = curr[1:] + c\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    return x,y\n",
    "\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "Xtr, Ytr  = build_dataset(names[:n1])\n",
    "Xval, Yval  = build_dataset(names[n1:n2])\n",
    "Xte, Yte  = build_dataset(names[n2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / (fan_in**0.5)\n",
    "        self.bias = torch.randn(fan_out) * 0.0 if bias else None\n",
    "\n",
    "    def __call__(self, i):\n",
    "        out = i @ self.weight\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, i):\n",
    "        out = torch.tanh(i)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Embed:\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        self.C = torch.randn((fan_in, fan_out))\n",
    "\n",
    "    def __call__(self, i):\n",
    "        out = self.C[i]\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.C]\n",
    "\n",
    "\n",
    "# only works for 3d\n",
    "class MyFlatten:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, i):\n",
    "        n = self.n\n",
    "        A, B, C = i.shape\n",
    "\n",
    "        out = i.view(A, B // n, C * n)\n",
    "        # if B // n == 1:\n",
    "        if B//n == 1:\n",
    "            out = out.squeeze(1)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.01):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        self.gamma = torch.ones(dim)  # multiply thing\n",
    "        self.beta = torch.zeros(dim)  # bias thing\n",
    "\n",
    "        self.rolling_mean = torch.zeros(dim)\n",
    "        self.rolling_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, v):\n",
    "\n",
    "        if self.training:\n",
    "            if v.ndim == 2:\n",
    "                dim = 0\n",
    "            elif v.ndim == 3:\n",
    "                dim = (0,1)\n",
    "            vmean = v.mean(dim,keepdim=True)\n",
    "            vvar = v.var(dim,keepdim=True)\n",
    "            out = (\n",
    "                self.gamma * (v -vmean  / torch.sqrt(vvar + self.eps)) + self.beta\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                self.rolling_mean = (\n",
    "                    self.rolling_mean * (1 - self.momentum) + self.momentum * vmean\n",
    "                )\n",
    "                self.rolling_var = (\n",
    "                    self.rolling_var * (1 - self.momentum) + self.momentum *vvar \n",
    "                )\n",
    "        else:\n",
    "            out = (\n",
    "                self.gamma\n",
    "                * (v - self.rolling_mean)\n",
    "                / torch.sqrt(self.rolling_var + self.eps)\n",
    "                + self.beta\n",
    "            )\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self,layer_array):\n",
    "        self.layers = layer_array\n",
    "        self.parameters = [p for l in layer_array for p in l.parameters()]\n",
    "\n",
    "        with torch.no_grad():\n",
    "           self.layers[-1].weight *= 0.1\n",
    "\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def __call__(self,x):\n",
    "        for l in self.layers:\n",
    "            x=l(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faf9c360050>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76579\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 27\n",
    "n_embd = 24\n",
    "\n",
    "# n_hidden = 200\n",
    "n_hidden = 128\n",
    "\n",
    "model = Sequential([\n",
    "    Embed(vocab_size, n_embd),\n",
    "    MyFlatten(2),\n",
    "    Linear(n_embd * 2, n_hidden, bias=False),\n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    MyFlatten(2),\n",
    "    Linear(n_hidden * 2, n_hidden, bias=False),\n",
    "    BatchNorm1d(n_hidden ),\n",
    "    Tanh(),\n",
    "    MyFlatten(2),\n",
    "    Linear(n_hidden * 2, n_hidden,bias=False),\n",
    "    BatchNorm1d(n_hidden),Tanh(),\n",
    "    Linear(n_hidden,vocab_size)\n",
    "])\n",
    "\n",
    "# parameters = [p for l in model for p in l.parameters()]  # + [C]\n",
    "    # for l in model[:-1]:\n",
    "    #     if isinstance(l, Linear):\n",
    "    #         l.W *= 5/3\n",
    "\n",
    "\n",
    "print(sum([p.nelement() for p in model.parameters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 3.1782338619232178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[352], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m x_curr \u001b[38;5;241m=\u001b[39m Xtr[btch]\n\u001b[1;32m      9\u001b[0m y_curr \u001b[38;5;241m=\u001b[39m Ytr[btch]\n\u001b[0;32m---> 10\u001b[0m x_curr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_curr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(x_curr,y_curr)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters: \n",
      "Cell \u001b[0;32mIn[330], line 14\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 14\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "hello = Embed(vocab_size,n_embd)\n",
    "# forward pass\n",
    "for i in range(max_steps):\n",
    "    btch = torch.randint(high=Xtr.shape[0], size=(batch_size,))\n",
    "    x_curr = Xtr[btch]\n",
    "    y_curr = Ytr[btch]\n",
    "    x_curr = model(x_curr)\n",
    "    out = torch.nn.functional.cross_entropy(x_curr,y_curr)\n",
    "\n",
    "    for p in model.parameters: \n",
    "        p.grad = None\n",
    "    out.backward()\n",
    "    lr = 0.1 if i < 150_000 else 0.01\n",
    "    for p in model.parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    if i % 10_000 == 0:\n",
    "        print(f'epoch {i}, loss: {out.item()}')\n",
    "    lossi.append(out.log10().item())\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 1068",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[341], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1000]' is invalid for input of size 1068"
     ]
    }
   ],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1,1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    l.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.680844306945801\n",
      "val 2.723149538040161\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\"train\": (Xtr, Ytr), \"val\": (Xval, Yval), \"test\": (Xte, Yte)}[split]\n",
    "    # emb = C[x]\n",
    "    # x = emb.view(emb.shape[0], -1)\n",
    "    # for layer in model:\n",
    "        # x = layer(x)\n",
    "    x = model(x)\n",
    "    loss = torch.nn.functional.cross_entropy(x, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city.\n",
      "asari.\n",
      "vaylan.\n",
      "soniyah.\n",
      "dricksa.\n",
      "gorrien.\n",
      "tazyvor.\n",
      "adonie.\n",
      "korie.\n",
      "dainze.\n",
      "gine.\n",
      "starra.\n",
      "aryzonnie.\n",
      "dhylal.\n",
      "uritamo.\n",
      "kashaunka.\n",
      "bordan.\n",
      "kestg.\n",
      "kigia.\n",
      "chuzem.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "\n",
    "    while True:\n",
    "        # emb =C[torch.tensor([context])]\n",
    "        # print(emb.shape)\n",
    "        # x = emb.view(-1,emb.shape[0] * emb.shape[1])\n",
    "        # x = emb.view(emb.shape[0] ,-1)\n",
    "        x = torch.tensor([context])\n",
    "        # for layer in model:\n",
    "            # print(x.shape)\n",
    "            # x = layer(x)\n",
    "        x = model(x)\n",
    "        logits = x\n",
    "        probs =torch.nn.functional.softmax(logits,dim=1)\n",
    "        ix = torch.multinomial(probs,num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
